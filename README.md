# Real-Time Crypto Market Analytics Platform

An end-to-end **real-time data engineering and analytics platform** that ingests live cryptocurrency trade data, processes it using **Kafka and Spark Structured Streaming**, stores analytics in **AWS S3 (data lake)**, and visualizes insights through a **Streamlit dashboard** querying data directly from S3 using **DuckDB**.

This project demonstrates modern **streaming data architecture**, **cloud-native analytics**, and **production-grade fault-tolerant pipelines**.

---

## Key Features

- Real-time crypto trade ingestion from Coinbase
- Kafka-based streaming pipeline
- Spark Structured Streaming with:
  - Event-time processing
  - Watermarking
  - Windowed aggregations
  - Fault tolerance via checkpoints
- Analytics stored as partitioned Parquet files in AWS S3
- DuckDB querying Parquet directly from S3 (no warehouse needed)
- Interactive Streamlit dashboard with auto-refresh
- Cloud deployment using Streamlit Cloud

---

## ğŸ—ï¸ System Architecture

```text
Coinbase API
     |
     v
Kafka Producer
     |
     v
Kafka Topic
     |
     v
Spark Structured Streaming
     |
     v
AWS S3 (Parquet Data Lake)
     |
     v
DuckDB
     |
     v
Streamlit Dashboard
```

## Tech Stack
#### Streaming & Processing
* Apache Kafka
* Apache Spark (Structured Streaming)

#### Storage & Analytics
* AWS S3 (Data Lake)
* Parquet
* DuckDB

#### Dashboard
* Streamlit
* Plotly

#### Infrastructure
* Docker & Docker Compose
* AWS
* Python 3.11+

## ğŸ“ Project Structure
Real-Time-Crypto-Market-Analytics/
â”‚
â”œâ”€â”€ dashboard/                     # Streamlit dashboard
â”‚   â”œâ”€â”€ .streamlit/
â”‚   â”‚   â””â”€â”€ secrets.toml           # AWS credentials (Streamlit Cloud)
â”‚   â”œâ”€â”€ app.py                     # Main dashboard app
â”‚   â””â”€â”€ test.py
â”‚
â”œâ”€â”€ ingestion/                     # Kafka producer
â”‚   â”œâ”€â”€ coinbase_producer.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ kafka/                         # Kafka configuration
â”‚
â”œâ”€â”€ spark/                         # Spark streaming jobs
â”‚   â”œâ”€â”€ analytics_trades.py
â”‚   â”œâ”€â”€ clean_trades.py
â”‚   â””â”€â”€ sql/
â”‚       â””â”€â”€ market_sql_analytics.py
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ analytics/                 # S3 analytics output
â”‚   â””â”€â”€ checkpoints/               # Spark checkpoints
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt



## ğŸ”„ Data Flow Explanation
**1. Ingestion**
* Live crypto trade data is pulled from Coinbase
* Data is produced to Kafka topics


**2. Stream Processing**
* Spark Structured Streaming consumes Kafka data
* Applies event-time windows and aggregations
* Handles late data using watermarks
* Writes aggregated analytics to S3 as Parquet


**3. Storage**
* Data stored in AWS S3
* Partitioned by symbol
* Optimized for analytics queries


**4. Analytics & Visualization**
* DuckDB queries Parquet files directly from S3
* Streamlit dashboard renders metrics and charts
* Auto-refresh enabled for near real-time updates


## ğŸ§ª Local Development Setup
1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/your-username/Real-Time-Crypto-Market-Analytics.git
cd Real-Time-Crypto-Market-Analytics
```

2ï¸âƒ£ Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3ï¸âƒ£ Start Kafka & Infrastructure
```bash
docker-compose up -d
```
